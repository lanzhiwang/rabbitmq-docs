# An end to synchrony: performance improvements in 3.3  同步的终结：3.3 中的性能改进

https://blog.rabbitmq.com/posts/2014/04/an-end-to-synchrony-performance-improvements-in-3-3/

*April 3, 2014*

Well, we got the [bad news](https://blog.rabbitmq.com/posts/2014/04/breaking-things-with-rabbitmq-3-3) out of the way yesterday, so today let’s talk about (some of) the good news: some types of publishing and consuming are now a great deal faster, especially in clusters.  好吧，昨天我们已经把坏消息排除在外，所以今天让我们谈谈（一些）好消息：某些类型的发布和消费现在要快得多，尤其是在集群中。

The various internal parts of RabbitMQ communicate by passing messages among themselves (both within nodes and across clusters); this is how Erlang applications work. It’s always been a design goal of RabbitMQ that operations which were asynchronous in AMQP (i.e. sending and receiving messages and acknowledgements) should be asynchronous inside the server. There’s a good reason for that: whenever you perform a synchronous operation you are limited by latency while you wait for the reply, so asynchrony is a route to much faster messaging.  RabbitMQ 的各个内部部分通过在它们之间传递消息（在节点内和跨集群）进行通信；这就是 Erlang 应用程序的工作方式。 RabbitMQ 的设计目标一直是 AMQP 中的异步操作（即发送和接收消息和确认）在服务器内部应该是异步的。这有一个很好的理由：每当您执行同步操作时，您在等待回复时都会受到延迟的限制，因此异步是实现更快消息传递的途径。

Unfortunately, while that’s always been a goal, we haven’t always hit it. In particular there were two holdouts where asynchronous messaging in AMQP became synchronous inside the server: mandatory publishing, and consuming messages with a prefetch limit set through `basic.qos`. These holdouts have been fixed in 3.3.0.  不幸的是，虽然这一直是一个目标，但我们并不总是能实现它。特别是在 AMQP 中的异步消息在服务器内部变得同步时，有两个问题：强制发布和使用通过 basic.qos 设置的预取限制的消息。这些保留已在 3.3.0 中修复。

As a refresher, mandatory publishing means **tell the publisher if its messages did not end up routed to any queues**, while consuming with a prefetch limit means **make sure you only send the consumer a maximum number of outstanding unacknowledged messages**.  作为复习，强制发布意味着告诉发布者它的消息是否最终没有路由到任何队列，而使用预取限制消费意味着确保您只向消费者发送最大数量的未确认消息。

So let’s look at some numbers…  那么让我们看一些数字……

### Mandatory publishing

|                       | 3.2.4           | 3.3.0            |
| :-------------------- | :-------------- | :--------------- |
| **Mandatory publish** | 5.0kHz balanced | 12.9kHz balanced |

*This test involved a two node cluster on a single machine, with a publisher connected to one node and a consumer connected to the other, with the queue located on the same node as the consumer. Messages were small and non-persistent, and neither acks nor confirms were used. The machine was a Dell Precision workstation, but the point is to look at relative performance change here.*  该测试涉及一台机器上的两个节点集群，一个发布者连接到一个节点，一个消费者连接到另一个节点，队列与消费者位于同一节点上。 消息很小且不持久，既没有使用acks，也没有使用confirms。 该机器是戴尔 Precision 工作站，但重点是在这里查看相对性能变化。

Hopefully you can see how badly synchrony hurts performance here. And remember that the performance penalty imposed by synchronous messaging is proportional to network latency - and these two nodes were located on the same machine, so a real cluster would have a worse drop off.  希望您能在这里看到同步对性能的严重影响。 请记住，同步消息传递带来的性能损失与网络延迟成正比——这两个节点位于同一台机器上，因此真正的集群会出现更严重的下降。

Note also that in both cases the sending and receiving rates were the same; messages were not backing up in the queue.  另请注意，在这两种情况下，发送和接收速率是相同的； 消息未在队列中备份。

### Consuming with a prefetch limit

We would expect that a high prefetch limit would give nearly the same performance as no prefetch limit, and that as we reduce the limit we will get lower performance, since at some points the queue will have to wait until the consumer acks a message before it can send another.  我们期望高预取限制将提供与无预取限制几乎相同的性能，并且随着我们降低限制，我们将获得较低的性能，因为在某些时候队列将不得不等到消费者在它之前确认消息可以再发一个。

|                         | 3.2.4                          | 3.3.0                         |
| :---------------------- | :----------------------------- | :---------------------------- |
| **No limit**            | 15.0kHz send / 11.0kHz receive | 15.8kHz balanced              |
| **prefetch_limit=1000** | 6.2kHz send / 3.6kHz receive   | 15.8kHz balanced              |
| **prefetch_limit=100**  | 6.2kHz send / 3.6kHz receive   | 13.5kHz balanced              |
| **prefetch_limit=10**   | 6.2kHz send / 3.6kHz receive   | 14.0kHz send / 7.0kHz receive |
| **prefetch_limit=1**    | 18.0kHz send / 0.9kHz receive  | 18.0kHz send / 0.9kHz receive |

*This test had the same characteristics as above except that the queue was on the same node as the publisher and acknowledgements were used when consuming.*  该测试具有与上述相同的特征，只是队列与发布者位于同一节点上，并且在消费时使用了确认。

There are several interesting effects visible in the numbers in this table:  在此表中的数字中可以看到几个有趣的效果：

- Even with the prefetch limit off, 3.3.0 was slightly faster, and prevented messages backing up. This is due to a new feature which I’ll talk about in a future blog post.  即使关闭了预取限制，3.3.0 也稍微快了一点，并且阻止了消息备份。这是由于我将在以后的博客文章中讨论的一项新功能。

- A sufficiently high prefetch limit (such that the queue never has to wait for the consumer) has no performance cost in 3.3.0, whereas any prefetch limit at all hurts performance in 3.2.4.  足够高的预取限制（使得队列永远不必等待消费者）在 3.3.0 中没有性能成本，而在 3.2.4 中任何预取限制都会损害性能。

- All of the prefetch limits between 10, 100 and 1000 had exactly the same (bad) performance in 3.2.4 - that’s because the limiting factor turns out to be the synchronous communication between the consuming channel and the queue.  10、100 和 1000 之间的所有预取限制在 3.2.4 中具有完全相同（差）的性能 - 这是因为限制因素原来是消费通道和队列之间的同步通信。

- Finally, when we reach a prefetch limit of 1, both 3.2.4 and 3.3.0 perform equally badly - that’s because the limiting factor has now become the amount of time we wait for the consumer to send an acknowledgement for a single message at a time.  最后，当我们达到 1 的预取限制时，3.2.4 和 3.3.0 的性能同样糟糕——这是因为限制因素现在变成了我们等待消费者在时间。

So with these changes the messaging internals of RabbitMQ are now asynchronous under all circumstances, bringing substantial performance benefits. It’s worth pointing out that the semantics for `basic.qos` [had to change slightly](https://www.rabbitmq.com/consumer-prefetch.html) for this to be possible, but this seems like a small price for such a large improvement.  因此，通过这些更改，RabbitMQ 的消息传递内部现在在所有情况下都是异步的，从而带来了巨大的性能优势。值得指出的是，为了实现这一点，basic.qos 的语义必须稍作改变，但对于如此大的改进来说，这似乎是一个很小的代价。


